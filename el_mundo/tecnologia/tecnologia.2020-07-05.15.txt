Google Twitter 
El polémico programa de ordenador que predice si eres un criminal por tu aspecto: "Tienes cara de malo"
A finales del siglo XIX, un oficial de policía francés, Alphonse Bertillon, se acercó al concepto de tecnología del reconocimiento facial. Bertillon creó un método para identificar criminales en función de sus características físicas. A cada persona se le adjudicaban una ficha en la que venían reflejadas 11 mediciones físicas, un retrato fotográfico y una descripción física por escrito.
Sin embargo, esta no fue la primera aproximación. En 1800, la frenología fue una pseudociencia pionera en decidir quien era malo o bueno por sus facciones y el tamaño de su cráneo. Popular entre los eugenistas, esta teoría fue célebre por reafirmar el racismo que ya había en la época, al pretender identificar diferencias como el tamaño de la cabeza o el ancho de la nariz, como prueba de su intelecto innato, virtud o criminalidad. A pesar del interés de algunos estudiosos de esta la frenología en extender su prestigio como supuesta ciencia, esta nunca llegó a tener consideración en los entornos universitarios.
Aunque parezcan vestigios de otra época, a principios de mayo, un comunicado de prensa de la Universidad de Harrisburg reavivó el debate sobre si nos delata la cara de malo, profundizando en el pensamiento de la maldad inherente desde el nacimiento. El texto afirmaba que dos profesores y un estudiante habían desarrollado un programa de reconocimiento facial que podía predecir si alguien sería un criminal, añadiendo además que sería publicado en Springer Nature.
"Con una precisión del 80 por ciento y sin prejuicios raciales, puede predecir si alguien es un criminal basándose únicamente en una imagen de su cara. El software está destinado a ayudar a la policía a prevenir el delito", reza el estudio titulado 'Una red neuronal profunda para predecir la criminalidad utilizando el procesamiento de imagen'. Al igual que en la película 'Minority Report', la presunción de inocencia pasa a ser presunción de culpabilidad. 
El modelo de red neuronal profunda en cuestión predice si alguien es un criminal basándose únicamente en una imagen de su cara. "Al automatizar la identificación de posibles amenazas sin prejuicios, nuestro objetivo es producir herramientas para la prevención del delito, la aplicación de la ley y para que las aplicaciones militares se vean menos afectadas por los prejuicios implícitos y las respuestas emocionales", unas afirmaciones que parecen sacadas de una novela Cyberpunk pero que fueron lanzadas este 2020.
Como respuesta, un grupo de 1.700 investigadores, sociólogos, historiadores y especialistas en la ética del aprendizaje automático hicieron publica una carta en la que condenaban este artículo. A continuación, Springer Nature confirmó en Twitter que no publicará la investigación.
La coalición de expertos, autodenominada Coalición por la tecnología crítica, destaca que las afirmaciones del estudio "se basan en premisas científicas, investigaciones y métodos poco sólidos que han sido desacreditados a lo largo de los años". Argumentan que es imposible predecir la criminalidad sin prejuicios raciales, "porque la categoría de 'criminalidad' en sí misma está sesgada racialmente".
Entre los referentes desacreditados que citan en la carta (mencionados en el estudio) se encuentra Cesare Lombroso, un criminólogo italiano del siglo XIX defensor del darwinismo social y la frenología que defendió la noción de que la criminalidad es heredada.
El extenso documento de denuncia subraya que tecnología de predicción de delitos reproduce injusticias y causa daños reales, haciendo referencia a que los casos recientes de sesgo algorítmico a través de la raza, la clase y el género han revelado una propensión estructural de los sistemas de aprendizaje automático para amplificar las formas históricas de discriminación.
¿Por qué repiten estos softwares los errores humanos? Si los datos utilizados para construir los algoritmos están sesgados, las predicciones de los algoritmos también estarán sesgadas. El documento firmado por los científicos argumenta que "debido a la naturaleza racialmente sesgada de la policía en los EEUU, cualquier algoritmo predictivo que modele la criminalidad solo reproducirá los prejuicios ya reflejados en el sistema de justicia penal".
Existen varios ejemplos anteriores de chascos éticos en cuanto a softwares de inteligencia artificial. En 2016, ingenieros de Stanford y Google refutaron las afirmaciones de un estudio de la Universidad Jiao Tong de Shanghai que decían tener un algoritmo que al igual que el mencionado al principio de este mismo artículo, podría predecir la criminalidad mediante el análisis facial.
Un año más tarde, varios investigadores de Stanford afirmaron que su software podría determinar si alguien es homosexual o heterosexual según su rostro. Organizaciones LGBTQ+ criticaron el estudio, señalando lo perjudicial y peligroso que podría ser una herramienta como esta en países que criminalizan la homosexualidad. 
Los padres, a menudo, ven a sus hijos como quieren que sean, no como son. Algo que puede extrapolarse a la creación de algoritmos de predicción. Creyendo en la objetividad de la tecnología, parece que por ahora es inevitable que comparta mirada con su creador o simplemente con la pasada historia contemporánea.
Conforme a los criterios deThe Trust Project
